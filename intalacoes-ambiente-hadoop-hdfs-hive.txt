Roteiro de instalação do Hadoop 3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pre requisito: instalacao do git
sudo apt install git

Abrir um terminal na máquina ubuntu


1. Instalação do ssh:

sudo apt install ssh


2. Instalação do pdsh

sudo apt install pdsh


3. Atualizar o arquivo de profile .bashrc e incluir a seguinte entrada:

nano ~/.bashrc

export PDSH_RCMD_TYPE=ssh

# Obs: control+O e enter para salvar.
#      control+X para sair


4. Criar uma chave ssh:

ssh-keygen -t rsa -P ""


5. Copiar conteudo da chave para o arquivo authorizted_keys:

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


6. Confirmar a configuração do ssh:

ssh localhost


7. Instalação do Java 8 (a versão necessariamente deve ser 8):

sudo apt install openjdk-8-jdk


8. Confirmar a instalação do Java 8:

java -version


9. Fazer o download dos binários do Hadoop 3.2.1: 

sudo wget -P ~ https://archive.apache.org/dist/hadoop/core/hadoop-3.2.1/hadoop-3.2.1.tar.gz


10. Descompactar o arquivo com os binários do hadoop

tar xzf hadoop-3.2.1.tar.gz


11. Renomear o diretório criado para hadoop:

mv hadoop-3.2.1 hadoop


12. Confirmar o diretório JAVA_HOME:

ls /usr/lib/jvm/java-8-openjdk-amd64/


13. Alterar o arquivo hadoop-env.sh e incluir a variável JAVA_HOME ao final do arquivo:

nano ~/hadoop/etc/hadoop/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/


14. Atualizar o arquivo core-site.xml:

nano ~/hadoop/etc/hadoop/core-site.xml

<configuration>    
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/hdata</value>
    </property>
</configuration>


15. Configurar o arquivo hdfs-site.xml:

nano ~/hadoop/etc/hadoop/hdfs-site.xml

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>


16. Configurar o mapred-site.xml:

nano ~/hadoop/etc/hadoop/mapred-site.xml

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
    </property>
</configuration>


17. Configurar o yarn-site.xml

nano ~/hadoop/etc/hadoop/yarn-site.xml

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property> 
</configuration>


18. Atualizar o arquivo de profile com as novas informações: 

nano .bashrc

export HADOOP_HOME=/home/hadoop/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}


19. Formatar o HDFS:

cd ~/hadoop
bin/hdfs namenode -format


20. Iniciar o hdfs

sbin/start-dfs.sh

Executar o comando jps para ver namenodes e datanodes rodando:

jps


21. Abrir o console do hdfs, abra o navegador e digite:

localhost:9870


22. Iniciar o yarn:

sbin/start-yarn.sh


23. Abrir console do yarn, acessar o brower e digitar:

localhost:8088

# reinicie a maquina

===========================================================================================================================
===========================================================================================================================

Roteiro de instalação do Hive 3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Pré requisito, hadoop em execução:

cd ~/hadoop/sbin
./start-all.sh


1. Fazer download dos binários do hive
cd ~
wget https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz


2. Extrair arquivos dos binários do hive
tar xzf apache-hive-3.1.2-bin.tar.gz

2.1. Renomear diretorio do hive:
mv apache-hive-3.1.2-bin hive


3. Atualizar variaveis de ambiente:

sudo nano .bashrc
 
# Adicao de variaveis do Hive
export HIVE_HOME=/home/hadoop/hive
export PATH=$PATH:$HIVE_HOME/bin
 

4. Atualizar variaveis de ambiente na memória
source ~/.bashrc


5. Incluir a variavel HADOOP_HOME no hive:

sudo nano $HIVE_HOME/bin/hive-config.sh
 
export HADOOP_HOME=/home/hadoop/hadoop


6. Criar diretórios padrão do hive no HDFS:

hdfs dfs -mkdir /tmp

hdfs dfs -chmod g+w /tmp

hdfs dfs -mkdir -p /user/hive/warehouse

hdfs dfs -chmod g+w /user/hive/warehouse


7. Atualizar .jar do guava (deve ser o mesmo para hadoop e hive

rm $HIVE_HOME/lib/guava-19.0.jar
cp $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar $HIVE_HOME/lib/
 

8. Criar metadados do Hive

cd $HIVE_HOME/bin/
schematool –initSchema –dbType derby


9. Inicializar o Hive

cd $HIVE_HOME/bin
 
hive
