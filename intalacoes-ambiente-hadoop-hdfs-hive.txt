Roteiro de instalação do Hadoop 3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Abrir um terminal na máquina ubuntu


1. Instalação do ssh:

sudo apt install ssh


2. Instalação do pdsh

sudo apt install pdsh


3. Atualizar o arquivo de profile .bashrc e incluir a seguinte entrada:

export PDSH_RCMD_TYPE=ssh

# Obs: control+O e enter para salvar.
#      control+X para sair


4. Criar uma chave ssh:

ssh-keygen -t rsa -P ""


5. Copiar conteudo da chave para o arquivo authorizted_keys:

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


6. Confirmar a configuração do ssh:

ssh localhost


7. Instalação do Java 8 (a versão necessariamente deve ser 8):

sudo apt install openjdk-8-jdk


8. Confirmar a instalação do Java 8:

java -version


9. Fazer o download dos binários do Hadoop 3.2.1: 

sudo wget -P ~ https://archive.apache.org/dist/hadoop/core/hadoop-3.2.1/hadoop-3.2.1.tar.gz


10. Descompactar o arquivo com os binários do hadoop

tar xzf hadoop-3.2.1.tar.gz


11. Renomear o diretório criado para hadoop:

mv hadoop-3.2.1 hadoop


12. Confirmar o diretório JAVA_HOME:

ls ls /usr/lib/jvm/java-8-openjdk-amd64/


13. Alterar o arquivo hadoop-env.sh e incluir a variável JAVA_HOME ao final do arquivo:

nano ~/hadoop/etc/hadoop/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/


14. Atualizar o arquivo core-site.xml:

nano ~/home/hadoop/hadoop/core-site.xml

<configuration>    
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/user/hdata</value>
    </property>
</configuration>


15. Configurar o arquivo hdfs-site.xml:

nano ~/hadoop/etc/hadoop/hdfs-site.xml

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>


16. Configurar o mapred-site.xml:

nano ~/hadoop/etc/hadoop/mapred-site.xml

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/home/user/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/home/user/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/home/user/hadoop</value>
    </property>
</configuration>


17. Configurar o yarn-site.xml

nano ~/hadoop/etc/hadoop/yarn-site.xml

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property> 
</configuration>


18. Atualizar o arquivo de profile com as novas informações: 

nano .bashrc

export HADOOP_HOME="/home/user/hadoop"
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin 
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}


19. Formatar o HDFS:

cd hadoop
bin/hdfs namenode -format


20. Iniciar o hdfs

sbin/start-dfs.sh

Executar o comando jps para ver namenodes e datanodes rodando:

jps


21. Abrir o console do hdfs, abra o navegador e digite:

localhost:9870


22. Iniciar o yarn:

sbin/start-yarn.sh


23. Abrir console do yarn, acessar o brower e digitar:

localhost:8088


